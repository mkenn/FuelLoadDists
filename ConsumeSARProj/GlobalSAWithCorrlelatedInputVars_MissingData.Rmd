---
title: 'Emissions Global Sensitivity Analysis: Draft methods'
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## General methods:

Sample from empirically estimated distributions, separately for each EVT Group. Requires a baseline fuel bed for those fuel types not in the data base.

Use Iman method to induce empirical **rank** correlation structure among the variables. Claims that the method creates a rank correlation matrix of the input variables similar to the estimated one, while preserving the marginal distributions and the properties of the sampling scheme used to obtain the input vectors (so general and distribution-free):

Let K = # input variables, N = sample size; R = NxK (rows, columns) matrix with each column an independent permutations of an arbitrary set a(i), i = 1, ..., N numbers (called "scores"). (*not sure yet what a(i) is*).

C* is the user-supplied rank correlation matrix, and let C = C*. P is the matrix such that PP' = C. P can be obtained by the Cholesky factorization scheme (?) used by Scheuer and Stoller (1962) (ref in Iman). Then R_iP' results in a vector that has the desired correlation matrix, and RP' = R* has the desired rank correlation matrix (or close to). 

The example they give, the scores a(i) are the van der Waerden scores $\phi^{-1}(i/(N+1))$, i = 1,...,N, in each column, where $\phi^{-1}$ is the inverse of the standard normal distribution. e.g., i = 1, then we need qnorm(1/(16)) when N = 15. The method then would be to randomly arrange the vector qnorm((1:N)/(N+1)) for each column. We can use "sample" for this

Their example:

```{r corrEx1}
# example rank correlation matrix, use cor(method=Spear,an)
C.star<-matrix(c(c(1,rep(0,5)),c(0,1,rep(0,4)),c(0,0,1,0,0,0),c(0,0,0,1,0.75,-0.70), 
               c(0,0,0,0.75,1,-0.95),c(0,0,0,-.7,-.95,1)),ncol=6,byrow=TRUE)
P.trans<-chol(C.star) # this returns the cholesky factorization, P' in the paper
P.mat<-t(P.trans) # The transpose of the above matrix is the P matrix in the paper
N<-15
a.vals<-qnorm((1:N)/(N+1))
K<-6
R.mat<-matrix(NA,ncol=K,nrow=N)
for(j in 1:K)
{
  R.mat[,j]<-sample(a.vals)
}
R.star<-R.mat%*%P.trans 
# so now we have a matrix with something like the correlation structure indicated by C*

mu.k<-c(1.5,0.6,2.8,3.5,1.8,0.1)
sigma.k<-c(0.4,0.1,0.9,0.5,0.25,0.8)
# Now generate desired sample structure, NxK, call X
# and arrange each column such that the rank of the observation
# in each row matches the rank in that row for R*. This
# preserves the RANK correlation structure
X.mat<-matrix(NA,ncol=K,nrow=N)
X.sort<-matrix(NA,ncol=K,nrow=N)
for(j in 1:K)
{
  X.mat[,j]<-rlnorm(N,mu.k[j],sigma.k[j])
  tmp.sort<-sort(R.star[,j],index.return=TRUE) 
# so tmp.sort$ix gives the row number for each rank, 
# i.e., tmp.sort$ix[i] gives the row number of the ith ordered statistic.
# inversely, which(tmp.sort$ix==i) gives the ordered statistic of the ith row
  tmp2.sort<-sort(X.mat[,j],index.return=TRUE) 
# so now we replace the ith row in X with the value of the same rank as 
# the ith row in R.star
  for(i in 1:N)
    X.sort[i,j]<-X.mat[tmp2.sort$ix[which(tmp.sort$ix==i)],j]
}
# Works in simple example!
# although note, especially with a small sample we get 
# some substantial correlation in the zero-correlation
# pairwise relationships

round(cor(X.sort,method="spearman"),digits=3)
C.star
```

Also Iman proposes a variance reduction method that produces closer (more consistent) correlation matrices. Let T = the realized correlation matrix for R above. Use Cholesky factorization to find Q such that T = QQ' (as with C and P above). We can then solve for S = $PQ^{-1}$. Then we have R*_b=RS'. Not sure if we necessarily need this.

```{r corWithVarRed}
T.mat<-cor(R.mat,method="spearman")
Q.trans<-chol(T.mat)
Q.mat<-t(T.mat)
S.mat<-P.mat%*%solve(T.mat)
R.star.b<-R.mat%*%t(S.mat)
X.sort.b<-matrix(NA,ncol=K,nrow=N)
for(j in 1:K)
{
  tmp.sort<-sort(R.star.b[,j],index.return=TRUE) 
# so tmp.sort$ix gives the row number for each rank, 
# i.e., tmp.sort$ix[i] gives the row number of the ith ordered statistic.
# inversely, which(tmp.sort$ix==i) gives the ordered statistic of the ith row
  tmp2.sort<-sort(X.mat[,j],index.return=TRUE) 
# so now we replace the ith row in X with the value of 
# the same rank as the ith row in R.star
  for(i in 1:N)
    X.sort.b[i,j]<-X.mat[tmp2.sort$ix[which(tmp.sort$ix==i)],j]
}
# Works in simple example!

round(cor(X.sort.b,method="spearman"),digits=3)
C.star

```

Now let's test this on an example EVT group. Some considerations--do we go with the full correlation matrix, or only those deemed "statistically" significant, and by what measure? Also, back to zeroes! We need to include them in the correlation consideration, and theoretically they shouldn't be a problem with a rank correlation v. a pearson correlation. 
Further complicating this analysis is the missing data. In particular, whether we can say that the missing data are random ("missing completely at random, MCAR"), or if there is some kind of systematic reason for the missing data. I don't think we have very good reason to say the data are missing at random. They are missing because they weren't measured. In general missing data literature is based on a single study, where missingness is due to things like subjects dropping out. Ours is a special case, where we're combining (presumably indepdendent) studies. So the missingness has to do with the choices of the researcher and the goals of the studies. A question is whether these have a chance to introduce bias (in general, I'm still ok with marginal distribution fitting, that is, individual distributions--this is mostly concerned with estimating the correlation matrix)

The issue is that if we estimate correlations pair-wise on only the non-missing data, with thereby different rows in the database contributing to the estimation depending on which pair of data we're considering, then the resulting correlation matrix is no longer positive semi-definite. That's a technical way of saying that it's not a proper matrix for any of these calculations, even if we have confidence in the individual pair-wise correlations. There are additional statistical considerations of bias and efficiency of the estimates. 

There are methods to handle missing data, including some form of imputation. might want to set some threshold for missingness, and perform reasonable imputations. Note, we're performing estimation, not inference here, so mostly concerned about bias. It depends a lot on if we can say that the data are missing at random (MAR), where bias is introduced in ESTIMATION if they're not missing at random. A complete-case approach is the simplest, most conservative approach, and works fine if MAR. Might want to do an analysis comparing some of the missing data with complete-case and some form of imputation. The imputation of course relies on the correlation structure of the complete-case, which is what we're trying to estimate. Since we're not performing estimation (of things like means and variances), not are we performing inference, maybe complete case is our best shot.

Can do multiple imputations, and take the mean correlation? Will this preserved the variance/covariance matrix?

Options: Review subsets of fuel types in original SA study design, and decide if these help to reduce missingness.

subsets: (where is duff?)
1. Ground fuels including course wood > 1000 hr, rotten or sound, stumps, basal accumultaion, and squirrel middens (so just cwd likely)
In database (?); (column index): "X10Khr_loading_Mgha" (3), "X10KhrR_loading_Mgha" (4),   "X10KhrS_loading_Mgha" (5), "X1Khr_loading_Mgha" (7), "X1KhrR_loading_Mgha" (8),      "X1KhrS_loading_Mgha" (9), "cwd_loading_Mgha" (10), "cwd_rotten_loading_Mgha" (11), "cwd_sound_loading_Mgha" (12), "GT10KhrR_loading_Mgha" (16), "GT10KhrS_loading_Mgha" (17)
Note: probably don't need 10Khr loading, or 1K hr, or cwd_loading WITH their rotten/sound components. Look at R/S without total, then just totals w/o rotten/sound. Note also that cwd is sum 1000hr, 10Khr, and >10Khr

So possible combinations that make sense are:
"X10KhrR_loading_Mgha" (4),   "X10KhrS_loading_Mgha" (5), "X1KhrR_loading_Mgha" (8),      "X1KhrS_loading_Mgha" (9), "GT10KhrR_loading_Mgha" (16), "GT10KhrS_loading_Mgha" (17)

Or:"X10Khr_loading_Mgha" (3), "X1Khr_loading_Mgha" (7), "GT10KhrR_loading_Mgha" (16), "GT10KhrS_loading_Mgha" (17)

Or simply: "cwd_rotten_loading_Mgha" (11), "cwd_sound_loading_Mgha" (12)



2. Surface fuels:
shrubs, herbs, fine woody, litter/lichen/moss, piles
In database (?): "X100hr_loading_Mgha" (1), "X10hr_loading_Mgha" (2), "X1hr_loading_Mgha" (6), "fwd_loading_Mgha" (15), "herb_loading_Mgha" (18), "lichen_loading_Mgha" (20), "litter_loading_Mgha" (22), "moss_loading_Mgha" (23), "shrub_loading_Mgha" (26)
Skip fwd, same logic as above, or include it in aggregate

3. Canopy fuels Overstory/midstory (really underrepresented in database)

```{r getData,echo=FALSE}
load("../DistFittingRProj/Workspaces/HurdleCustomFitsNoZeroWNoOut.RData")
# should contain everything necessary to estimate correlation matrices
# Make sure that the woody totals are present
data.file$X10Khr_loading_Mgha<-data.file$X10KhrR_loading_Mgha+data.file$X10KhrS_loading_Mgha
data.file$X1Khr_loading_Mgha<-data.file$X1KhrR_loading_Mgha+data.file$X1KhrS_loading_Mgha

data.file$cwd_loading_Mgha<-data.file$X10Khr_loading_Mgha+data.file$X1Khr_loading_Mgha+data.file$GT10KhrR_loading_Mgha+data.file$GT10KhrS_loading_Mgha


data.file$fwd_loading_Mgha<-data.file$X1hr_loading_Mgha+data.file$X10hr_loading_Mgha+data.file$X100hr_loading_Mgha

# so now we need a new function, to calculate a complete and proper
# correlation matrix on the subsets of interest in the database
smolder1.id<-c(4,5,8,9,16,17)
smolder2.id<-c(3,7,16,17) # because we don't seem to have a total column for >10K

flaming1.id<-c(1,2,6,18,20,22,23,26)
flaming2.id<-c(15,18,20,22,23,26) # assuming fwd is 1-hr+10-hr+100-hr
flaming3.id<-c(1,2,6,18,22,26) # remove lichen and moss because those data seem to be rare

# now we will subset by the groups for an example EVT, and calculate the
# correlation matrix. It does seem reasonable that if 1 in this group is measured,
# the others likely also are
# try with the first EVT
#replace commented section below with function call
#######
cur.evt<-3
tmp.df<-data.file[data.file[,EVTCol]==evt.vals[cur.evt],]
tmp.loads.df<-tmp.df[,start.col:ncol(tmp.df)]
# might need an imputation package to assess the missingness 
# in the subset of data
tmp2.loads.df<-tmp.loads.df[,smolder1.id]

# package mice has a way to assess the missingness in a matrix
library(mice)
tmp.md<-md.pattern(tmp2.loads.df)
# looks like it shows, in order, the number of rows for which they're all present,
# then the number of each of the different combinations of the variables
# so, if the number of rows of this object is > 3, then we
# have a hole in the matrix, otherwise, we do not
# need to update sampling from hurdle distribution
# replace commented code below with function calls
##########
# min.co.occur<-10
# if(dim(tmp.md)[1]<=3)
#   {
#     tmp.total<-as.numeric(row.names(tmp.md)[1]) # gives the number of observations that occur in all columns
#     if(tmp.total>min.co.occur)
#       # so, if no holes, then can eliminate the NA's with any given column
#         {
#       cur.loads<-tmp2.loads.df[!is.na(tmp2.loads.df[,1]),]
#         # and then estimate the correlation matrix
#         cur.corr<-cor(cur.loads,method="spearman")
#         
#         P.trans<-chol(cur.corr) # this returns the cholesky factorization, P' in the paper # getting not positive definite error, seems that we still need a smaller matrix; exclude those variables with ALL zeroes
#         # So, this isn't working because we are handling the NA's pairwise, removing them 
#         # for each pair. This means that the resulting correlation matrix is NOT
#         # positive semi-definite, which means that the factorization can't work.
#         P.mat<-t(P.trans) # The transpose of the above matrix is the P matrix in the paper 
#         N<-1000 # 1000 samples
#         a.vals<-qnorm((1:N)/(N+1))
#         K<-ncol(cur.corr) # the number of variables
#         R.mat<-matrix(NA,ncol=K,nrow=N)
#         for(j in 1:K)
#         {
#           R.mat[,j]<-sample(a.vals)
#         }
#         R.star<-R.mat%*%P.trans 
#         # and now we use the hurdle fits to draw random values, then 
#         # apply this matrix to enforce the correlation structure
#         
#         tmp.fits<-distributionCustomFittingHurdleNOut$HurdleFit[[cur.evt]][smolder1.id,]
#         tmp.ranks<-distributionCustomRankingHurdleNOut[[cur.evt]][smolder1.id,]
#         # so now we have a new issue, one of the columns is so heavy with 0,
#         # the distribution wasn't estimated because the remaining values didn't
#         # satisfy the minimum threshold. So, what do we assume now? For those variables, draw a random number of zeroes based on prop0, and then what for non-zeroes?
#         # might be reasonable to just estimate a lognormal for these in general,
#         # since we know in general they're R-skewed, and we don't have good information
#         # on the shape beyond that...
#         sens.mat<-matrix(NA,ncol=length(smolder1.id),nrow=N)
#         for(k in 1:length(smolder1.id))
#         {
#           if(!is.na(tmp.ranks$dist1.fit[k]))
#           {
#             tmp.distr<-switch(tmp.ranks$dist1.fit[k],
#                               lnormLL="lnorm",
#                               gammaLL="gamma")
#             tmp.samp<-simHurdle.fn(distr=tmp.distr,prop0=tmp.fits$prop0[k],nsamp=N,nrep=1,
#                                  param1=tmp.fits[k,paste(tmp.distr,".p1",sep="")],
#                                  param2=tmp.fits[k,paste(tmp.distr,".p2",sep="")])
#             sens.mat[,k]<-tmp.samp[,1]
#           }
#           else
#           {
#             tmp.loads<-cur.loads[,k]
#             cur.prop0<-length(tmp.loads[tmp.loads==0])/length(tmp.loads)
#             if(cur.prop0<1)
#             {
#                 cur.gt0<-tmp.loads[tmp.loads>0]
#                 tmp.mu<-mean(log(cur.gt0))
#                 tmp.sd<-sd(log(cur.gt0))
#                 tmp.samp<-simHurdle.fn(distr="lnorm",prop0=cur.prop0,nsamp=N,nrep=1,
#                                  param1=tmp.mu,
#                                  param2=tmp.sd)
#                 sens.mat[,k]<-tmp.samp[,1]
# 
#               
#             }
#             else
#               sens.mat[,k]<-0
#           }
#         }
#     }
# }
# else{
#   # so we have holes in the matrix. Option 1, whole-case--at some threshold of
#   # proportion missing, just use the whole cases. 
#   # otherwise we would impute the values based on relationships with others
#   # but, for example, the missing smoldering fuels seem to be zero
#         cur.loads<-tmp2.loads.df[!is.na(tmp2.loads.df[,1])&!is.na(tmp2.loads.df[,2])&!is.na(tmp2.loads.df[,3])&!is.na(tmp2.loads.df[,4])&!is.na(tmp2.loads.df[,5])&!is.na(tmp2.loads.df[,6]),]
#         if(nrow(cur.loads)>min.co.occur)
#         {
#                   cur.corr<-cor(cur.loads,method="spearman")
#         
#         P.trans<-chol(cur.corr) # this returns the cholesky factorization, P' in the paper # getting not positive definite error, seems that we still need a smaller matrix; exclude those variables with ALL zeroes
#         # So, this isn't working because we are handling the NA's pairwise, removing them 
#         # for each pair. This means that the resulting correlation matrix is NOT
#         # positive semi-definite, which means that the factorization can't work.
#         P.mat<-t(P.trans) # The transpose of the above matrix is the P matrix in the paper 
#         N<-1000 # 1000 samples
#         a.vals<-qnorm((1:N)/(N+1))
#         K<-ncol(cur.corr) # the number of variables
#         R.mat<-matrix(NA,ncol=K,nrow=N)
#         for(j in 1:K)
#         {
#           R.mat[,j]<-sample(a.vals)
#         }
#         R.star<-R.mat%*%P.trans 
#         # and now we use the hurdle fits to draw random values, then 
#         # apply this matrix to enforce the correlation structure
#         
#         tmp.fits<-distributionCustomFittingHurdleNOut$HurdleFit[[cur.evt]][smolder1.id,]
#         tmp.ranks<-distributionCustomRankingHurdleNOut[[cur.evt]][smolder1.id,]
#         # so now we have a new issue, one of the columns is so heavy with 0,
#         # the distribution wasn't estimated because the remaining values didn't
#         # satisfy the minimum threshold. So, what do we assume now? For those variables, draw a random number of zeroes based on prop0, and then what for non-zeroes?
#         # might be reasonable to just estimate a lognormal for these in general,
#         # since we know in general they're R-skewed, and we don't have good information
#         # on the shape beyond that...
#         sens.mat1<-matrix(NA,ncol=length(smolder1.id),nrow=N)
#         for(k in 1:length(smolder1.id))
#         {
#           if(!is.na(tmp.ranks$dist1.fit[k]))
#           {
#             tmp.distr<-switch(tmp.ranks$dist1.fit[k],
#                               lnormLL="lnorm",
#                               gammaLL="gamma")
#             tmp.samp<-simHurdle.fn(distr=tmp.distr,prop0=tmp.fits$prop0,nsamp=N,nrep=1,
#                                  param1=tmp.fits[k,paste(tmp.distr,".p1",sep="")],
#                                  param2=tmp.fits[k,paste(tmp.distr,".p2",sep="")])
#             sens.mat1[,k]<-tmp.samp[,1]
#           }
#           else
#           {
#             tmp.loads<-cur.loads[,k]
#             cur.prop0<-length(tmp.loads[tmp.loads==0])/length(tmp.loads)
#             if(cur.prop0<1)
#             {
#               cur.gt0<-tmp.loads[tmp.loads>0]
#                 tmp.mu<-mean(log(cur.gt0))
#                 tmp.sd<-sd(log(cur.gt0))
#                 tmp.samp<-simHurdle.fn(distr="lnorm",prop0=cur.prop0,nsamp=N,nrep=1,
#                                  param1=tmp.mu,
#                                  param2=tmp.sd)
#                 sens.mat1[,k]<-tmp.samp[,1]
# 
#               
#             }
#             else
#               sens.mat[,k]<-0
#           }
#         }
#         }
# 
# }
# # and now try to enforce the correlation structure
# # works! Now to find a missing example
# X.sort<-matrix(NA,ncol=K,nrow=N)
# for(j in 1:length(smolder1.id))
# {
#   tmp.sort<-sort(R.star[,j],index.return=TRUE) 
# # so tmp.sort$ix gives the row number for each rank, 
# # i.e., tmp.sort$ix[i] gives the row number of the ith ordered statistic.
# # inversely, which(tmp.sort$ix==i) gives the ordered statistic of the ith row
#   tmp2.sort<-sort(sens.mat1[,j],index.return=TRUE) 
# # so now we replace the ith row in X with the value of the same rank as 
# # the ith row in R.star
#   for(i in 1:N)
#     X.sort[i,j]<-sens.mat1[tmp2.sort$ix[which(tmp.sort$ix==i)],j]
# }
# 
# # now with the variance reduction method
# T.mat<-cor(R.mat,method="spearman")
# Q.trans<-chol(T.mat)
# Q.mat<-t(T.mat)
# S.mat<-P.mat%*%solve(T.mat)
# R.star.b<-R.mat%*%t(S.mat)
# X.sort.b<-matrix(NA,ncol=K,nrow=N)
# for(j in 1:K)
# {
#   tmp.sort<-sort(R.star.b[,j],index.return=TRUE) 
# # so tmp.sort$ix gives the row number for each rank, 
# # i.e., tmp.sort$ix[i] gives the row number of the ith ordered statistic.
# # inversely, which(tmp.sort$ix==i) gives the ordered statistic of the ith row
#   tmp2.sort<-sort(sens.mat1[,j],index.return=TRUE) 
# # so now we replace the ith row in X with the value of 
# # the same rank as the ith row in R.star
#   for(i in 1:N)
#     X.sort.b[i,j]<-sens.mat1[tmp2.sort$ix[which(tmp.sort$ix==i)],j]
# }
# 
########
source("../Functions/SimHurdleDist_FN.R")
source("../Functions/SensitivitySampleWithCorr_FN.R")
sens.mats.smolder.list<-corr.sa.fn(data.file,fuel.ids=smolder1.id,complete.case=TRUE,min.co.occur=30,
                     evts=evt.vals,EVTCol=EVTCol,start.col=start.col,n.samp=1000,
                     rankObj=distributionCustomRankingHurdleNOut,fitObj=distributionCustomFittingHurdleNOut)
sens.mats.smolder.list.upper<-corr.sa.fn(data.file,fuel.ids=smolder1.id,complete.case=TRUE,min.co.occur=30,
                     evts=evt.vals,EVTCol=EVTCol,start.col=start.col,n.samp=1000,
                     rankObj=distributionCustomRankingHurdleNOut,fitObj=distributionCustomFittingHurdleNOut,
                     upper.quantile = 0.95)
# looks like lichen and moss might present an issue here. Try without
# might also be more of an issue with complete cases
sens.mats.flame.list<-corr.sa.fn(data.file,fuel.ids=flaming3.id,complete.case=TRUE,min.co.occur=30,
                     evts=evt.vals,EVTCol=EVTCol,start.col=start.col,n.samp=1000,
                     rankObj=distributionCustomRankingHurdleNOut,fitObj=distributionCustomFittingHurdleNOut)
sens.mats.flame.list.upper<-corr.sa.fn(data.file,fuel.ids=flaming3.id,complete.case=TRUE,min.co.occur=30,
                     evts=evt.vals,EVTCol=EVTCol,start.col=start.col,n.samp=1000,
                     rankObj=distributionCustomRankingHurdleNOut,fitObj=distributionCustomFittingHurdleNOut,
                     upper.quantile=0.95)
# new issue when there is zero variability in one of the fuel types (e.g., all 0)
# then the correlation is undefined
# strategy: exclude from SA and include as that value in baseline
# Last issue--when filling in some of the distributions with insufficient non-zero coverage,
# we're getting NAs in the lnorm draw.
```

Sensitivity analysis sampling procedure, hurdle distribution.

1. Divide fuel types into smoldering and flaming. Perform global SA on those sub-types, keeping everything else at baseline

2. Estimate correlation matrix for complete cases within that subgroup. (assumes MAR missingness, but seems more justifiable than imputation) (set minimum number of complete cases (30 for now)) (for variables that have no variability, e.g., all zeroes, the correlation matrix cannot be estimated--for these just give the non-varying value)

3. Sample estimated hurdle distributions (marginal) individually for each fuel type in the subgroup. (draw a random number of zeroes based on prop0, then from the estimated non-zero distribution for the remaining values--fit to the full data set for each individual observation, not the complete cases) (if the distribution could not be estimated because the insufficient number of non-zeroes, then just sample with replacement from the non-zeroes)(if the fuel type is only 0 for this evt, assign it only zeroes)(Seem to be getting very long tails on the distributions, might consider truncating for sake of SA--to 95th percentile random draws--looks better)

4. Sort the marginal distributions (as above) so that the resulting matrix approximates the desired correlation structure.

5. Use this matrix to perform sensitivity analysis (map to consume and fofem inputs)

Outstanding issues--first guess that this will fall apart under Sobol SA procedure (which itself re-assembles the matrices). Might need to look at alternatives like partial rank correlation coefficients
At first subjective glance, Sobol seems to preserve grosser structure (with possibly muted correlations). This is because the correlation structure is preserved for the columns not rearranged. We see in general relatively high positive and low negative correlations where we expect them, and negligible correlations where we expect them.

To try sobol, we divide the above matrix in 2, and have these be the basis for the Sobol rearrangement

So: Perform Sobol SA both with and without correlation structure, then 1 more both with and without correlation structure. Compare sensitivity

Let's count the evts for which there might be holes in this subset

```{r countMissing}
missing.smolder<-rep(NA,length(evt.vals))

for(cur.evt in 1:length(evt.vals))
{
  tmp.df<-data.file[data.file[,EVTCol]==evt.vals[cur.evt],]
  tmp.loads.df<-tmp.df[,start.col:ncol(tmp.df)]
# might need an imputation package to assess the missingness 
# in the subset of data
  tmp2.loads.df<-tmp.loads.df[,smolder1.id]

# package mice has a way to assess the missingness in a matrix
  library(mice)
  tmp.md<-md.pattern(tmp2.loads.df)
  missing.smolder[cur.evt]<-dim(tmp.md)[1] # anything>3 indicates missing values
}
table(missing.smolder)
```
It might be that a lot of these missing values are 0--so can we really impute them? Back to whole case only, maybe. 

Here are the indices, and the corresponding evts, with incomplete smoldering data:
> which(missing.smolder>3)
 [1]  6  7  8  9 10 11 13 14 15 16 17 19 20 21 23 24 25 26 27 28 29 30 31 33 34 35 43 44 58 66
> evt.vals[which(missing.smolder>3)]
 [1] 602 603 604 607 610 612 614 615 617 620 622 625 626 627 629 630 631 632 633 635 638 639 640 643 644 645 660 661 682 693