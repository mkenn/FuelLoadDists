Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Lilliefors1967,
abstract = {The standard tables used for the Kolmogorov-Smirnov test are valid when testing whether a set of observations are from a completely specified continuous distribution. If one or more parameters must be estimated from the sample thein the tables are no longer valid. A table is given in this note for use with the Kolmogorov-Smirnov statistic for testing whether a set of observations is from a normal population when the mean and variance are not specified but must be estimated from the sample. The table is obtained from a Monte Carlo calculation. A brief Monte Carlo investigation is made of the power of the test.},
author = {Lilliefors, H W},
file = {:D$\backslash$:/MendeleyDocs/Lilliefors{\_}KS{\_}MC.pdf:pdf},
journal = {American Statistical Journal},
keywords = {normality,statistics},
number = {318},
pages = {399--402},
title = {{On the Kolmogorov-Smirnov test for normality with mean and variance unknown}},
volume = {June},
year = {1967}
}
@article{Xiao-Hua1999,
abstract = {In this paper, we consider the problem of testing the mean equality of several independent populations that contain log-normal and possibly zero observations. We first showed that the currently used methods in statistical practice, including the nonparametric Kruskal–Wallis test, the standard ANOVA F-test and its two modified versions, the Welch test and the Brown–Forsythe test, could have poor Type I error control. Then we propose a likelihood ratio test that is shown to have much better Type I error control than the existing methods. Finally, we analyze two real data sets that motivated our study using the proposed test.},
author = {Xiao-Hua, Zhou and Tu, Wanzhu},
doi = {10.1111/j.0006-341X.1999.00645.x},
file = {:D$\backslash$:/MendeleyDocs/Xiao-Hua{\_}et{\_}al-1999-Biometrics.pdf:pdf},
isbn = {0006341X},
issn = {0006341X},
journal = {Biometrics},
keywords = {Cost data,Log-normal,Maximum likelihood,Skewed distribution,Zero costs},
number = {2},
pages = {645--651},
title = {{Comparison of Several Independent Population Means When Their Samples Contain Log-Normal and Possibly Zero Observations}},
url = {http://doi.wiley.com/10.1111/j.0006-341X.1999.00645.x},
volume = {55},
year = {1999}
}
@article{Punt2000,
abstract = {The methods used to develop catch rate based indices of relative abundance for the school shark Galeorhinus galeus resource off southern Australia are outlined. These methods are based on fitting generalized linear models to catch and effort data for several regions in this fishery. This is to take account of the multi-gear nature of the fishery and the spatial structure of the trends in catch rate. The data on whether or not the catch rate is zero and the catch rate given that it is non-zero are analysed separately and then combined to provide indices of abundance. The former analysis is based on assuming the data are Bernoulli random variables. Given the uncertainty about the appropriate error-model to assume when fitting generalized linear models to catch and effort data, four alternative error-models - log-normal, log-gamma, Poisson, and negative binomial were explored when modelling the non-zero catch rates. (C) 2000 Elsevier Science B.V.},
author = {Punt, Andr{\'{e}} E. and Walker, Terence I. and Taylor, Bruce L. and Pribac, Fred},
doi = {10.1016/S0165-7836(99)00106-X},
file = {:D$\backslash$:/MendeleyDocs/PuntEtAlCPUE.pdf:pdf},
isbn = {01657836 (ISSN)},
issn = {01657836},
journal = {Fisheries Research},
keywords = {CPUE,Catch rates,Galeorhinus galeus,Shark,Standardization},
number = {2},
pages = {129--145},
pmid = {10359564},
title = {{Standardization of catch and effort data in a spatially-structured shark fishery}},
volume = {45},
year = {2000}
}
@article{Min2002,
abstract = {Applications in which data take nonnegative values but have a substantial proportion of values at zero occur in many dis- ciplines. The modeling of such “clumped-at-zero” or “zero-inflated” data is challenging. We survey models that have been proposed. We consider cases in which the response for the non-zero observations is continuous and in which it is discrete. For the continuous and then the discrete case, we review models for analyzing cross-sectional data. We then summarize extensions for repeated measurement analyses (e.g., in longitudinal studies), for which the literature is still sparse. We also mention applications in which more than one clump can oc- cur and we suggest problems for future research.},
author = {Min, Yongyi and Agresti, Alan},
file = {:D$\backslash$:/MendeleyDocs/min{\_}agresti{\_}2002.pdf:pdf},
journal = {Jirss},
keywords = {and phrases,compliance,finite mixture model,logistic regression,model,neyman type a distribution,proportional odds model,semicontinuous data,tobit,zero-inflated data},
number = {May},
pages = {7--33},
title = {{Modeling Nonnegative Data with Clumping at Zero : A Survey Models for Semicontinuous Data}},
volume = {1},
year = {2002}
}
@article{Fletcher2005,
abstract = {We discuss a method for analyzing data that are positively skewed and contain a substantial proportion of zeros. Such data commonly arise in ecological applications, when the focus is on the abundance of a species. The form of the distribution is then due to the patchy nature of the environment and/or the inherent heterogeneity of the species. The method can be used whenever we wish to model the data as a response variable in terms of one or more explanatory variables. The analysis consists of three stages. The first involves creating two sets of data from the original: one shows whether or not the species is present; the other indicates the logarithm of the abundance when it is present. These are referred to as the ‘presence data' and the ‘log-abundance' data, respectively. The second stage involves modelling the presence data using logistic regression, and separately modelling the log-abundance data using ordinary regression. Finally, the third stage involves combining the two models in order to estimate the expected abundance for a specific set of values of the explanatory variables. A common approach to analyzing this sort of data is to use a ln (y+c) transformation, where c is some constant (usually one). The method we use here avoids the need for an arbitrary choice of the value of c, and allows the modelling to be carried out in a natural and straightforward manner, using well-known regression techniques. The approach we put forward is not original, having been used in both conservation biology and fisheries. Our objectives in this paper are to (a) promote the application of this approach in a wide range of settings and (b) suggest that parametric bootstrapping be used to provide confidence limits for the estimate of expected abundance.},
author = {Fletcher, David and MacKenzie, Darryl and Villouta, Eduardo},
doi = {10.1007/s10651-005-6817-1},
file = {:D$\backslash$:/MendeleyDocs/Fletcher{\_}et{\_}al{\_}2005.pdf:pdf},
isbn = {1352-8505},
issn = {13528505},
journal = {Environmental and Ecological Statistics},
keywords = {Abundance,Bootstrap,Conditional model,Ecklonia,Evechinus},
number = {1},
pages = {45--54},
pmid = {204},
title = {{Modelling skewed data with many zeros: A simple approach combining ordinary and logistic regression}},
volume = {12},
year = {2005}
}
@article{Welsh1996,
abstract = {We consider several statistical models for the analysis of the abundance of a rare species and these are illustrated in detail with data for the abundance of Leadbeater's Possum in montane ash forests of south-eastern Australia. These data are characterised by a discrete distribution with the zero class inflated. In many statistical problems the parameters of this distribution depend on covariates, such as the number of hollow bearing trees. We advocate a conditional model which is simple to interpret and readily fitted. We show how to obtain standard errors for the parameter estimates. We also show how to estimate the mean abundance of animals at a site. The methods outlined in this paper offer a powerful framework for the study of problems having a discrete response (like abundance) with the zero class inflated.},
author = {Welsh, A.H. and Cunningham, R.B. and Donnelly, C.F. and Lindenmayer, D.B.},
doi = {10.1016/0304-3800(95)00113-1},
file = {:D$\backslash$:/MendeleyDocs/WelshEtal.pdf:pdf},
isbn = {0304-3800},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {abundance models,density,model comparison,population,statistical models},
number = {1-3},
pages = {297--308},
pmid = {1162},
title = {{Modelling the abundance of rare species: statistical models for counts with extra zeros}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0304380095001131},
volume = {88},
year = {1996}
}
@article{Lanchenbruch2002,
abstract = {This paper is an introduction to a set of papers on two-part models that were presented at the 2000 Joint Statistical Meetings of American Statistical Association, International Biometric Society (ENAR), International Biometric Society (WNAR), Institute of Mathematical Statistics, and Statistical Society of Canada. The second part of the paper summarizes results of a study of the size and power of two-part models. It proposes that two-part models are a useful alternative to the t-test and the Wilcoxon test.},
author = {Lanchenbruch, Peter A.},
doi = {10.1191/0962280202sm289ra},
file = {:D$\backslash$:/MendeleyDocs/Lachenbruch.pdf:pdf},
isbn = {0962-2802 (Print)},
issn = {09622802},
journal = {Statistical Methods in Medical Research},
number = {4},
pages = {297--302},
pmid = {12197297},
title = {{Analysis of data with excess zeros}},
volume = {11},
year = {2002}
}
@article{STEFANSSON1996,
abstract = {This paper describes a method for the analysis of groundfish survey data by incorporating zero and non-zero values into a single model. This is done by using a model which modifies the delta-distribution approach to fit into the GLM framework and uses maximum likelihood to estimate parameters. No prior assumptions of homogeneity are used for the structure of the zero or non-zero values. The method is primarily applicable to fixed-station designs, although extensions to other designs are possible. The maximum likelihood estimation reduces to fitting a GLM to 0/1 values and another GLM to the positive abundance values. The new model is tested on Icelandic groundfish survey data. It is seen that the model can be used for evaluating the effect of different factors on catch rates as well as estimating abundance indices. Results from different models are compared on the basis of tuned VPA runs.},
author = {STEFANSSON, G},
doi = {10.1006/jmsc.1996.0079},
file = {:D$\backslash$:/MendeleyDocs/stefansson{\_}1996.pdf:pdf},
isbn = {1054-3139},
issn = {10543139},
journal = {ICES Journal of Marine Science},
keywords = {121 reykjav{\'{i}}k,accepted 1 june 1995,box 1390,generalized linear models,gunnar stef{\'{a}}nsson,iceland,marine research institute,marine surveys,o,p,received 7 september 1994,sk{\'{u}}lagata 4},
number = {3},
pages = {577--588},
title = {{Analysis of groundfish survey abundance data: combining the GLM and delta approaches}},
url = {https://academic.oup.com/icesjms/article-lookup/doi/10.1006/jmsc.1996.0079},
volume = {53},
year = {1996}
}
@article{Lecomte2013,
abstract = {* Ecological data such as biomasses often present a high proportion of zeros with possible skewed positive values. The Delta-Gamma (DG) approach, which models separately the presence–absence and the positive biomass, is commonly used in ecology. A less commonly known alternative is the compound Poisson-gamma (CPG) approach, which essentially mimics the process of capturing clusters of biomass during a sampling event.$\backslash$n$\backslash$n$\backslash$n* Regardless of the approach, the effort involved in obtaining a sample (henceforth called the sampling volume, but could also include swept areas, sampling durations, etc.), which can potentially be quite variable between samples, needs to be taken into account when modelling the resulting sample biomass. This is achieved empirically for the DG approach (using a generalized linear model with sampling volume as a covariate), and theoretically for the CPG approach (by scaling a parameter of the model). In this study, the consequences of this disparity between approaches were explored first using theoretical arguments, then using simulations and finally by applying the approaches to catch data from a commercial groundfish trawl fishery.$\backslash$n$\backslash$n$\backslash$n* The simulation study results point out that the DG approach can lead to poor estimates when far from standard idealized sampling assumptions. On the contrary, the CPG approach is much more robust to variable sampling conditions, confirming theoretical predictions. These results were confirmed by the case study for which model performances were weaker for the DG.$\backslash$n$\backslash$n$\backslash$n* Given the results, care must be taken when choosing an approach for dealing with zero-inflated continuous data. The DG approach, which is easily implemented using standard statistical softwares, works well when the sampling volume variability is small. However, better results were obtained with the CPG model when dealing with variable sampling volumes.},
author = {Lecomte, Jean Baptiste and Beno{\^{i}}t, Hugues P. and Ancelet, Sophie and Etienne, Marie Pierre and Bel, Liliane and Parent, Eric},
doi = {10.1111/2041-210X.12122},
file = {:D$\backslash$:/MendeleyDocs/Lecomte{\_}et{\_}al-2013-Methods{\_}in{\_}Ecology{\_}and{\_}Evolution.pdf:pdf},
isbn = {2041-210X},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Commercial fishery catches,Compound Poisson,Estimation of biomass,Sampling variability,Two-part model},
number = {12},
pages = {1159--1166},
title = {{Compound Poisson-gamma vs. delta-gamma to handle zero-inflated continuous data under a variable sampling volume}},
volume = {4},
year = {2013}
}
